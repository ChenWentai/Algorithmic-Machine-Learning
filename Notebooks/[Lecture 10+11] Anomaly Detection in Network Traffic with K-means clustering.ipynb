{"nbformat_minor": 0, "nbformat": 4, "cells": [{"source": ["<div>\n", "<h1>Run the cell below to generate the road map (do not modify it)</h1></div>"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": ["%%javascript\n", "var kernel = IPython.notebook.kernel;var thename = window.document.getElementById(\"notebook_name\").innerHTML;var command = \"THE_NOTEBOOK = \" + \"'\"+thename+\"'\";kernel.execute(command);command=\"os.environ['THE_NOTEBOOK'] = THE_NOTEBOOK\";kernel.execute(command);var cell = IPython.notebook.get_cell(2);cell.execute();IPython.notebook.get_cell(3).focus_cell();var x = $('.code_cell');$(x[1]).children('.input').hide();"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": ["outputdir = \"/tmp/tools/\"\n", "!mkdir -p $outputdir\n", "!wget \"https://www.dropbox.com/s/4g0pigmro4vo1b4/menutemplate?dl=0\" -O /tmp/tools/menutemplate >> /tmp/toollog 2>&1 \n", "!wget \"https://www.dropbox.com/s/3flttpzhsja8td7/construct_menu.py?dl=0\" -O /tmp/tools/construct_menu.py >> /tmp/toollog 2>&1 \n", "!python /tmp/tools/construct_menu.py \"{THE_NOTEBOOK}.ipynb\" {outputdir}\n", "from IPython.core.display import HTML\n", "output_file_name = outputdir + THE_NOTEBOOK.replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \".ipynb.html\"\n", "with open(output_file_name) as fp:\n", "    html = fp.read()\n", "HTML(html)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["# Anomaly Detection in Network Traffic with K-means clustering\n", "\n", "We can categorize machine learning algorithms into two main groups: **supervised learning** and **unsupervised learning**. With supervised learning algorithms, in order to predict unknown values for new data, we have to know the target value for many previously-seen examples. In contrast, unsupervised learning algorithms explore the data which has no target attribute to find some intrinsic structures in them.\n", "\n", "Clustering is a technique for finding similar groups in data, called **clusters**. Clustering is often called an unsupervised learning task as no class values denoting an a priori grouping of the data instances are given.\n", "\n", "In this notebook, we will use K-means, a very well known clustering algorithm to detect anomaly network connections based on statistics about each of them. For a thorough overview of K-means clustering, from a research perspective, have a look at this wonderful [tutorial](http://theory.stanford.edu/~sergei/slides/kdd10-thclust.pdf)."], "cell_type": "markdown", "metadata": {}}, {"source": ["## Goals\n", "We expect students to:\n", "* Learn (or revise) and understand the K-means algorithm\n", "* Implement a simple K-means algorithm\n", "* Use K-means to detect anomalies network connection data"], "cell_type": "markdown", "metadata": {}}, {"source": ["## Steps\n", "\n", "1. In section 1, we will have an overview about K-means then implement a simple version of it.\n", "2. In section 2, we build models with and without categorical features.\n", "3. Finally, in the last section, using our models, we will detect unsual connections."], "cell_type": "markdown", "metadata": {}}, {"source": ["# 1. K-means\n", "## 1.1. Introduction\n", "Clustering is a typical and well-known type of unsupervised learning. Clustering algorithms try to find natural groupings in data. Similar data points (according to some notion of similarity) are considered in the same group. We call these groups **clusters**.\n", "\n", "K-Means clustering is a simple and widely-used clustering algorithm. Given value of $k$, it tries to build $k$ clusters from samples in the dataset. Therefore, $k$ is an hyperparameter of the model. The right value of $k$ is not easy to determine, as it highly depends on the data set and the way that data is featurized.\n", "\n", "To measue the similarity between any two data points, K-means requires the definition of a distance funtion between data points. What is a distance? It is a value that indicates how close two data points are in their space. In particular, when data points lie in a $d$-dimensional space, the Euclidean distance is a good choice of a distance function, and is supported by MLLIB.\n", "\n", "In K-means, a cluster is a group of points, with a representative entity called a centroid. A centroid is also a point in the data space: the center of all the points that make up the cluster. It's defined to be the arithmetic mean of the points. In general, when working with K-means, each data sample is represented in a $d$-dimensional numeric vector, for which it is easier to define an appropriate distance function. As a consequence, in some applications, the original data must be transformed into a different representation, to fit the requirements of K-means.\n", "\n", "## 1.2. How does it work ?\n", "Given $k$, the K-means algorithm works as follows:\n", "\n", "1. Randomly choose $k$ data points (seeds) to be the initial centroids\n", "2. Assign each data point to the **closest centroid**\n", "3. Re-compute (update) the centroids using the current cluster memberships\n", "4. If a convergence criterion is not met, go to step 2\n", "\n", "We can also terminate the algorithm when it reaches an iteration budget, which yields an approximate result.\n", "From the pseudo-code of the algorithm, we can see that K-means clustering results can be sensitive to the order in which data samples in the data set are explored. A sensible practice would be to run the analysis several times, randomizing objects order; then, average the cluster centres of those runs and input the centres as initial ones for one final run of the analysis."], "cell_type": "markdown", "metadata": {}}, {"source": ["## 1.3. Illustrative example\n", "One of the best ways to study an algorithm is trying implement it.\n", "In this section, we will go step by step to implement a simple K-means algorithm."], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "### Question 1\n", "\n", "#### Question 1.1\n", "Complete the below function to calculate an euclidean distance between any two points in $d$-dimensional data space"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "import numpy as np\n", "\n", "# calculate distance between two d-dimensional points\n", "def euclidean_distance(p1, p2):\n", "    return ...\n", "\n", "# test our function\n", "assert (round(euclidean_distance([1,2,3] , [10,18,12]), 2) == 20.45), \"Function's wrong\"\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 1.2\n", "Given a data point and the current set of centroids, complete the function below to find the index of the closest centroid for that data point."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "def find_closest_centroid(datapoint, centroids):\n", "    # find the index of the closest centroid of the given data point.\n", "    ...\n", "\n", "assert(find_closest_centroid( [1,1,1], [ [2,1,2], [1,2,1], [3,1,2] ] ) == 1), \"Function's wrong\"\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 1.3\n", "Write a function to randomize $k$ initial centroids."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "np.random.seed(22324)\n", "\n", "# randomize initial centroids\n", "def randomize_centroids(data, k):\n", "    centroids = ...\n", "    ...\n", "    return centroids\n", "\n", "assert(len(\n", "    randomize_centroids(\n", "        np.array([ \n", "            np.array([2,1,2]), \n", "            np.array([1,2,1]), \n", "            np.array([3,1,2]) \n", "             ]), \n", "        2)) == 2), \"Wrong function\"\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 1.4\n", "Write function `check_converge` to check the stop creteria of the algorithm."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "MAX_ITERATIONS = ...\n", "\n", "# return True if clusters have converged , otherwise, return False  \n", "def check_converge(centroids, old_centroids, num_iterations, threshold=0):\n", "    # if it reaches an iteration budget\n", "    ...\n", "    # check if the centroids don't move (or very slightly)\n", "    ...\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 1.5\n", "Write function `update_centroid` to update the new positions for the current centroids based on the position of their members."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "# centroids: a list of centers\n", "# cluster: a list of k elements. Each element i-th is a list of data points that are assigned to center i-th\n", "def update_centroids(centroids, cluster):\n", "    ...\n", "    return centroids\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 1.6\n", "Complete the K-means algorithm scheleton below, with the functions you wrote above."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "# data : set of data points\n", "# k : number of clusters\n", "# centroids: initial list of centroids\n", "def kmeans(data, k=2, centroids=None):\n", "    \n", "    # randomize the centroids if they are not given\n", "    if not centroids:\n", "        centroids = ...\n", "\n", "    old_centroids = centroids[:]\n", "\n", "    iterations = 0\n", "    while True:\n", "        iterations += 1\n", "\n", "        # init empty clusters\n", "        clusters = [[] for i in range(k)]\n", "\n", "        # assign each data point to the closest centroid\n", "        for ...:\n", "            # find the closest center of each data point\n", "            centroid_idx = ...\n", "            \n", "            # assign datapoint to the closest cluster\n", "            clusters...\n", "        \n", "        # keep the current position of centroids before changing them\n", "        old_centroids = ...\n", "        \n", "        # update centroids\n", "        centroids = ...\n", "        \n", "        # if the stop criteria are met, stop the algorithm\n", "        if ...\n", "            ...\n", "    \n", "    return centroids\n", "```\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["Next, we will test our algorithm on [Fisher's Iris dataset](http://en.wikipedia.org/wiki/Iris_flower_data_set), and plot the resulting clusters in 3D."], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 1.7\n", "The code below can be used to test your algorithm with three different datasets: Iris, Moon and Blob.\n", "Run your algorithm to cluster datapoints in these datasets, plot the results and discuss about them. Do you think that our algorithm works well? Why?"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": ["# the sourcecode in this cell is inspired from \n", "# https://gist.github.com/bbarrilleaux/9841297\n", "\n", "%matplotlib inline\n", "\n", "from sklearn import datasets, cluster\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from mpl_toolkits.mplot3d import Axes3D\n", "\n", "\n", "# load data\n", "iris = datasets.load_iris()\n", "X_iris = iris.data\n", "y_iris = iris.target\n", "# do the clustering\n", "centers = kmeans(X_iris, k=3)\n", "labels = [find_closest_centroid(p, centers) for p in X_iris]\n", "\n", "#plot the clusters in color\n", "fig = plt.figure(1, figsize=(8, 8))\n", "plt.clf()\n", "ax = Axes3D(fig, rect=[0, 0, 1, 1], elev=8, azim=200)\n", "plt.cla()\n", "ax.scatter(X_iris[:, 3], X_iris[:, 0], X_iris[:, 2], c=labels)\n", "\n", "# moon\n", "# np.random.seed(0)\n", "# X, y = datasets.make_moons(2000, noise=0.2)\n", "\n", "# blob\n", "# np.random.seed(0)\n", "# X, y = datasets.make_blobs(n_samples=2000, centers=3, n_features=20, random_state=0)\n", "\n", "# centers = kmeans(X, k=3)\n", "# labels = [find_closest_centroid(p, centers) for p in X]\n", "\n", "# fig = plt.figure(1, figsize=(8, 8))\n", "# plt.clf()\n", "# plt.scatter(X[:,0], X[:,1], s=40, c=labels, cmap=plt.cm.Spectral)\n", "\n", "ax.w_xaxis.set_ticklabels([])\n", "ax.w_yaxis.set_ticklabels([])\n", "ax.w_zaxis.set_ticklabels([])\n", "ax.set_xlabel('Petal width')\n", "ax.set_ylabel('Sepal length')\n", "ax.set_zlabel('Petal length')\n", "\n", "plt.show()\n", "\n", "# Here we use sci-kit learn implementation of K-means\n", "# centers =cluster.KMeans(n_clusters=3)\n", "# centers.fit(X_iris) \n", "# labels = centers2.labels_\n"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["<div class=\"comment\">\n", "\n", "\n", "PUT YOUR COMMENT HERE !!!\n", "\n", "</div>"], "cell_type": "markdown", "metadata": {}}, {"source": ["That's enough about K-means for now. In the next section, we will apply MMLIB's K-means on Spark to deal with a large data in the real usecase.\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 2. Usecase: Network Intrusion \n", "\n", "Some attacks attempt to flood a computer with network traffic. In some other cases, attacks attempt to exploit flaws in networking software in order to gain unauthorized access to a computer. Detecting an exploit in an incredibly large haystack of network requests is not easy.\n", "\n", "Some exploit behaviors follow known patterns such as scanning every port in a short of time, sending a burst of request to a port... However, the biggest threat may be the one that has never been detected and classified yet. Part of detecting potential network intrusions is detecting anomalies. These are connections that aren't known to be attacks, but, do not resemble connections that have been observed in the past.\n", "\n", "In this notebook, K-means is used to detect anomalous network connections based on statistics about each of them.\n", "\n", "\n", "## 2.1. Data\n", "The data comes from [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html). The dataset is about 708MB and contains about 4.9M connections. For each connection, the data set contains information like the number of bytes sent, login attempts, TCP errors, and so on. Each connection is one line of CSV-formatted data, containing 38 features: back, buffer_overflow, ftp_write, guess_passwd, imap, ipsweep, land, loadmodule, multihop, neptune, nmap, normal, perl, phf, pod, portsweep, rootkit, satan, smurf, spy, teardrop, warezclient, warezmaster. For more details about each features, please follow this [link](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html).\n", "\n", "Many features take on the value 0 or 1, indicating the presence or absence of a behavior such as `su_attempted` in the 15th column. Some features are counts, like `num_file_creations` in the 17th columns. Some others are the number of sent and received bytes."], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.2. Clustering without using categorical features\n", "\n", "First, we need to import some packages that are used in this notebook."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 33, "cell_type": "code", "source": ["import os\n", "import sys\n", "import re\n", "from pyspark import SparkContext\n", "from pyspark import SparkContext\n", "from pyspark.sql import SQLContext\n", "from pyspark.sql.types import *\n", "from pyspark.sql import Row\n", "from pyspark.sql.functions import *\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "import numpy as np\n", "import pyspark.sql.functions as func\n", "import matplotlib.patches as mpatches\n", "from pyspark.mllib.clustering import KMeans, KMeansModel\n", "\n", "input_path = \"/datasets/k-means/kddcup.data\"\n", "raw_data = sc.textFile(input_path, 12)"], "outputs": [], "metadata": {"collapsed": true}}, {"source": ["### 2.2.1. Loading data"], "cell_type": "markdown", "metadata": {"collapsed": false}}, {"source": ["There are two types of features: numerical features and categorical features.\n", "Currently, to get familiar with the data and the problem, we only use numerical features. In our data, we also have pre-defined groups for each connection, which we can use later as our \"ground truth\" for verifying our results.\n", "\n", "**Note 1**: we don't use the labels in the training phase !!!\n", "\n", "**Note 2**: in general, since clustering is un-supervised, you don't have access to ground truth. For this reason, several metrics to judge the quality of clustering have been devised. For a short overview of such metrics, follow this [link](https://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation). Note that computing such metrics, that is trying to assess the quality of your clustering results, is as computationally intensive as computing the clustering itself!"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 2\n", "Write function `parseLine` to construct a tuple of `(label, vector)` for each connection, extract the data that contains only the data points (without label), then print the number of connections.\n", "\n", "Where,\n", "\n", "* `label` is the pre-defined label of each connection\n", "* `vector` is a numpy array that contains values of all features, but the label and the categorial features at index `1,2,3` of each connection. Each `vector` is a data point."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "def parseLine(line):\n", "    cols = line...\n", "    # label is the last column\n", "    label = ...\n", "    \n", "    # vector is every column, except the label\n", "    vector = ...\n", "    \n", "    # delete values of columns that have index 1->3 (categorical features)\n", "    ...\n", "    \n", "    # convert each value from string to float\n", "    vector = np.array(...)\n", "    \n", "    return (label, vector)\n", "\n", "labelsAndData = raw_data.map(...)\n", "\n", "# we only need the data, not the label\n", "data = labelsAndData...cache()\n", "\n", "# number of connections\n", "n = data....\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 3\n", "Using K-means algorithm of MLLIB, cluster the connections into two groups then plot the result. Why two groups? In this case, we are just warming up, we're testing things around, so \"two groups\" has no particular meaning.\n", "\n", "You can use the following parameters:\n", "\n", "* `maxIterations=10`\n", "* `runs=10`\n", "* `initializationMode=\"random\"`\n", "\n", "Discuss the result from your figure."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "clusters = KMeans....\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class=\"answer\">\n", "\n", "\n", "PUT YOUR ANSWER HERE !!!\n", "\n", "\n", "</div>"], "cell_type": "markdown", "metadata": {}}, {"source": ["### 2.2.3. Evaluating model"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 4\n", "\n", "One of the simplest method to evaluate our result is calculate the Within Set Sum of Squared Errors (WSSSE), or simply, 'Sum of Squared Errors'. An error of a data point is defined as it's distance to the closest cluster center.\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "from operator import add\n", "\n", "# Evaluate clustering by computing Within Set Sum of Squared Errors\n", "def error(clusters, point):\n", "    closest_center = ...\n", "    return ...\n", "\n", "WSSSE = data.map(...).reduce(...)\n", "print(\"Within Set Sum of Squared Error = \" + str(WSSSE))\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 5\n", "This is a good opportunity to use the given labels to get an intuitive sense of what went into these two clusters, by counting the labels within each cluster. Complete the following code that uses the model to assign each data point to a cluster, and counts occurrences of cluster and label pairs.\n", "What do you think about the result ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "clusterLabelCount = ...\n", "\n", "for item in clusterLabelCount:\n", "    print(item)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='comment' >\n", "\n", "PUT YOUR COMMENT HERE\n", "\n", "</div>"], "cell_type": "markdown", "metadata": {}}, {"source": ["### 2.2.4. Choosing K\n", "\n", "How many clusters are appropriate for a dataset? In particular, for our own dataset, it's clear that there are 23 distinct behavior patterns in the data, so it seems that k could be at least 23, or likely, even more. In other cases, we even don't have any information about the number of patterns at all (remember, generally your data is not labelled!). Our task now is finding a good value of $k$. For doing that, we have to build and evaluate models with different values of $k$. A clustering could be considered good if each data point were near to its closest centroid. One of the ways to evaluate a model is calculating the Mean of Squared Errors of all data points."], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 6\n", "Complete the function below to calculate the MSE of each model that is corresponding to each value of $k$.\n", "Plot the results. From the obtained result, what is the best value for $k$ ? Why ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "# k: the number of clusters\n", "def clusteringScore(data, k):\n", "    clusters = KMeans....\n", "    # calculate mean square error\n", "    return ...\n", "\n", "scores = ...\n", "for score in scores:\n", "    print(score)\n", "    \n", "# plot results\n", "plt...\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class=\"answer\">\n", "\n", "PUT YOUR ANSWER HERE !\n", "\n", "</div>"], "cell_type": "markdown", "metadata": {}}, {"source": ["### 2.2.5 Normalizing features\n", "\n", "K-means clustering treats equally all dimensions/directions of the space and therefore tends to produce more or less spherical (rather than elongated) clusters. In this situation, leaving variances uneven is equivalent to putting more weight on variables with smaller variance, so clusters will tend to be separated along variables with greater variance.\n", "\n", "In our notebook, since Euclidean distance is used, the clusters will be influenced strongly by the magnitudes of the variables, especially by outliers. Normalizing will remove this bias. \n", "\n", "Each feature can be normalized by converting it to a standard score. This means subtracting the mean of the feature\u2019s values from each value, and dividing by the standard deviation\n", "\n", "$normalize_i=\\frac{feature_i - \\mu_i}{\\sigma_i}$\n", "\n", "Where,\n", "\n", "* $normalize_i$ is the normalized value of feature $i$\n", "* $\\mu_i$ is the mean of feature $i$\n", "* $\\sigma_i$ is the standard deviation of feature $i$\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 7\n", "\n", "Complete the code below to normalize the data. Print the first 5 lines of the new data.\n", "\n", "HINT: If $\\sigma_i = 0$ then $normalize_i=feature_i - \\mu_i$"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "def normalizeData(data):\n", "    # number of connections\n", "    n = ...\n", "\n", "    # calculate the sum of each feature\n", "    sums = ...\n", "    print(sums)\n", "\n", "    # calculate means\n", "    means = ...\n", "\n", "    # calculate the sum square of each feature\n", "    sumSquares = ...\n", "    print(sumSquares)\n", "\n", "    # calculate standard deviation of each feature\n", "    stdevs = ...\n", "    print(stdevs)\n", "\n", "    def normalize(point):\n", "        return ...\n", "\n", "    return data.map(normalize)\n", "\n", "normalizedData = normalizeData(data).cache()\n", "print(normalizedData.take(5))\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 8\n", "Using the new data, build different models with different values of $k \\in [60,70,80,90,100,110]$. Evaluate the results by plotting them and choose the best value of $k$."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "scores = ...\n", "for score in scores:\n", "    print(score)\n", "\n", "plt...\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='answer'>\n", "\n", "PUT YOUR ANSWER HERE !!!\n", "\n", "</div>"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 9\n", "Plot the clustering result to see the difference between before and after normalizing features. Discuss about the difference and explain why and if normalization was useful."], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class=\"comment\">\n", "\n", "\n", "PUT YOUR ANSWER HERE !!!\n", "\n", "</div>"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.3. Clustering using categorical features"], "cell_type": "markdown", "metadata": {}}, {"source": ["### 2.3.1 Loading data\n", "\n", "In the previous section, we ignored the categorical features of our data: this is not a good idea, since these categorical features can be important in providing useful information for clustering.\n", "The problem is that K-means (or at least, the one we have developed and the one we use from MLLib) only work with data points in a metric space. Informally, this means that operations such as addition, substraction and computing the mean of data points are trivial and well defined. For a more formal definition of what a metric space is, follow this [link](https://en.wikipedia.org/wiki/Metric_space#Definition).\n", "\n", "What we will do next is to transform each categorical features into one or more numerical features. This approach is very widespread: imagine for example you wanted to use K-means to cluster text data. Then, the idea is to transform text data in $d$-dimensional vectors, and a nice way to do it is to use [word2vec](http://deeplearning4j.org/word2vec). If you're interested, follow this link to a nice [blog post](http://bigdatasciencebootcamp.com/posts/Part_3/clustering_news.html) on the problem.\n", "\n", "There are two approaches:\n", "\n", "* **Approach 1**: mapping **one** categorial feature to **one** numerical feature. The values in each categorical feature are encoded into unique numbers of the new numerical feature. For example, ['VERY HOT','HOT', 'COOL', 'COLD', 'VERY COLD'] will be encoded into [0,1,2,3,4,5]. However, by using this method, we implicit assume that the value of 'VERY HOT' is smaller than 'HOT'... This is not generally true.\n", "\n", "* **Approach 2**: mapping mapping **one** categorial feature to **multiple** numerical features. Basically, a single variable with $n$ observations and $d$ distinct values, to $d$ binary variables with $n$ observations each. Each observation indicating the presence (1) or absence (0) of the $d^{th}$ binary variable. For example, ['house', 'car', 'tooth', 'car'] becomes \n", "```\n", "[\n", "[1,0,0,0],\n", "[0,1,0,0],\n", "[0,0,1,0],\n", "[0,0,0,1],\n", "]\n", "```\n", "\n", "We call the second approach \"one-hot encoding\". By using this approach, we keep the same role for all values of categorical features.\n", "\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 10\n", "Calculate the number of distinct categorical features value (at index `1,2,3`). Then construct a new input data using one-hot encoding for these categorical features (don't throw away numerical features!)."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "def parseLineWithHotEncoding(line):\n", "    cols = line...\n", "    # label is the last column\n", "    label = ...\n", "    \n", "    vector = cols[0:-1]\n", "    \n", "    # the binary features that are encoded from the first categorial feature\n", "    featureOfCol1 = ...\n", "    # the binary features that are encoded from the second categorial feature\n", "    featureOfCol2 = ...\n", "    # the binary features that are encoded from the third categorial feature\n", "    featureOfCol3 = ...\n", "    \n", "    # construct the new vector\n", "    vector = ([vector[0]] + featureOfCol1 + featureOfCol2 + \n", "        featureOfCol3 + vector[4:])\n", "    \n", "    # convert each value from string to float\n", "    vector = np.array(...)\n", "    \n", "    return (label, vector)\n", "\n", "labelsAndData = raw_data.map(parseLine)\n", "\n", "# we only need the data, not the label\n", "data = labelsAndData.values().cache()\n", "\n", "\n", "normalizedData = normalizeData(data).cache()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["### 2.3.2. Building models"], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 11\n", "Using the new data, cluster the connections with different values of $k \\in [80,90,100,110,120,130,140,150,160]$.\n", "Evaluate the results and choose the best value of $k$ as previous questions."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "scores = list(...)\n", "plt....\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class=\"answer\">\n", "\n", "\n", "PUT YOUR ANSWER HERE !!!\n", "\n", "\n", "</div>"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.4. Anomaly detection\n", "When we have a new connection data (e.g., one that we never saw before), we simply find the closest cluster for it, and use this information as a proxy to indicate whether the data point is anomalous or not. A simple approach to decide wheter there is an anomaly or not, amounts to measuring the new data point\u2019s distance to its nearest centroid. If this distance exceeds some thresholds, it is anomalous."], "cell_type": "markdown", "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 12\n", "Build your model with the best value of $k$ in your opinion. Then, detect the anomalous connections in our data. Plot and discuss your result.\n", "\n", "HINT: The threshold has strong impact on the result. Be careful when choosing it! A simple way to choose the threshold's value is picking up a distance of a data point from among known data. For example, the 100th-farthest data point distance can be an option."], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 13\n", "Try other methods to find the best value for $k$ such as `silhouette`, `entropy`... In particular, with this data, you can take advantage of predefined labels to calculate the quality of model using entropy... However, we suggest you to try with `silhouette`. It's more general and can work with any dataset (with and without predefined labels).\n", "\n", "Here are some additional information about the metrics we suggest to use:\n", "- [Silhouette](https://en.wikipedia.org/wiki/Silhouette_(clustering))\n", "- [Hack approach to Silhouette](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n", "- [Entropy](http://scikit-learn.org/stable/modules/clustering.html) [Lookup for entropy]\n", "\n", "Note: you are free to play with any relevant evaluation metric you think appropriate for your work!"], "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 14\n", "Implement K-means on Spark so that It can work with large datasets in parallel. Test your algorithm with our dataset in this notebook. Compare our algorithm with the algorithm from MLLIB.\n", "\n", "Let's clarify the meaning of this question: what we want is for students to design the K-means algorithm for the parallel programming model exposed by Spark. You are strongly invited to use the Python API (pyspark). So, at the end of the day, you will operate on RDDs, and implement a \"map/reduce\" algorithm that performs the two phases of the standard K-means algorithm, i.e. the assignment step and the update step."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [], "outputs": [], "metadata": {"collapsed": true}}], "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "2.7.11", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}