{"nbformat_minor": 0, "nbformat": 4, "cells": [{"source": ["<div>\n", "<h1>Run the cell below to generate the road map (do not modify it)</h1></div>"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": ["%%javascript\n", "var kernel = IPython.notebook.kernel;var thename = window.document.getElementById(\"notebook_name\").innerHTML;var command = \"THE_NOTEBOOK = \" + \"'\"+thename+\"'\";kernel.execute(command);command=\"os.environ['THE_NOTEBOOK'] = THE_NOTEBOOK\";kernel.execute(command);var cell = IPython.notebook.get_cell(2);cell.execute();IPython.notebook.get_cell(3).focus_cell();var x = $('.code_cell');$(x[1]).children('.input').hide();"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": ["outputdir = \"/tmp/tools/\"\n", "!mkdir -p $outputdir\n", "!wget \"https://www.dropbox.com/s/4g0pigmro4vo1b4/menutemplate?dl=0\" -O /tmp/tools/menutemplate >> /tmp/toollog 2>&1 \n", "!wget \"https://www.dropbox.com/s/3flttpzhsja8td7/construct_menu.py?dl=0\" -O /tmp/tools/construct_menu.py >> /tmp/toollog 2>&1 \n", "!python /tmp/tools/construct_menu.py \"{THE_NOTEBOOK}.ipynb\" {outputdir}\n", "from IPython.core.display import HTML\n", "output_file_name = outputdir + THE_NOTEBOOK.replace(\" \", \"\").replace(\"[\", \"\").replace(\"]\", \"\") + \".ipynb.html\"\n", "with open(output_file_name) as fp:\n", "    html = fp.read()\n", "HTML(html)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["# Goals of the Laboratory\n", "In this introductory laboratory, we expect students to:\n", "\n", "1. Acquire basic knowledge about Python and Matplotlib\n", "2. Gain familiarity with Juypter Notebooks\n", "3. Gain familiarity with the PySpark API and SparkSQL\n", "\n", "To achieve such goals, we will go through the following steps:\n", "\n", "1. In section 1, **IPython** and **Jupyter Notebooks** are introduced to help students understand the environment used to work on Data Science projects.\n", "\n", "2. In section 2, we briefly overview **Python** and its syntax. In addition, we cover **Matplotlib**, a very powerful library to plot figures in Python, which you can use for your Data Science projects. Finally, we introduce **Pandas**, a python library that is very helpful when working on Data Science projects.\n", "\n", "3. In section 3 we cover the **PySpark** and **SparkSQL** APIs\n", "\n", "4. In section 4, we conclude the introductory laboratory with a simple use case.\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 1. Python, IPython and Jupyter Notebooks\n", "\n", "**Python** is a high-level, dynamic, object-oriented programming language. It is a general purpose language, which means that many things are made easy. It's designed to be easy to program and easy to read.\n", "\n", "**IPython** (Interactive Python) is orignally developed for Python. Now, it is a command shell for interactive computing in multiple programming languages. It offers rich media, shell syntax, tab completion, and history. IPython is based on an architecture that provides parallel and distributed computing. IPython enables parallel applications to be developed, executed, debugged and monitored interactively.\n", "\n", "**Jupyter Notebooks** are a web-based interactive computational environment for creating IPython notebooks. An IPython notebook is a JSON document containing an ordered list of input/output cells which can contain code, text, mathematics, plots and rich media.  That makes data analysis easier to perform, understand and reproduce. All laboratories in this course are prepared as Notebooks. As you can see, in this Notebook, we can put text, images, hyperlinks, source code... The Notebooks can be converted to a number of open standard output formats (HTML, HTML presentation slides, LaTeX, PDF, ReStructuredText, Markdown, Python) through `File` -> `Download As` in the web interface. Beside, Jupyter manages the notebooks' versions through a `checkpoint` mechanism. You can create checkpoint anytime via `File -> Save and Checkpoint`.\n", "\n", "Let's go throught the features of Jupyter Notebooks."], "cell_type": "markdown", "metadata": {}}, {"source": ["## 1.1. Tab completion\n", "\n", "Tab completion is a convenient way to explore the structure of any object you're dealing with. Simply type object_name.<TAB> to view the suggestion for object's attributes. Besides Python objects and keywords, tab completion also works on file and directory names."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": ["s = \"test function of tab completion\"\n", "\n", "# type s.<TAB> to see the suggestions\n", "# For example, you can show your tests to work on a string. Try splitting a string into its constituent words!"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 1.2. System shell commands\n", "\n", "To run any command in the system shell, simply prefix it with `!`. For example:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 6, "cell_type": "code", "source": ["# list all file and directories in the current folder\n", "!ls"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 1.3. Magic functions\n", "\n", "IPython has a set of predefined `magic functions` that you can call with a command line style syntax. There are two types of magics, line-oriented and cell-oriented. \n", "\n", "**Line magics** are prefixed with the `%` character and work much like OS command-line calls: they get as an argument the rest of the line, *where arguments are passed without parentheses or quotes*. \n", "\n", "**Cell magics** are prefixed with a double `%%`, and they are functions that get as an argument not only the rest of the line, but also the lines below it in a separate argument."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 7, "cell_type": "code", "source": ["%timeit range(1000)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 8, "cell_type": "code", "source": ["%%timeit x = range(10000)\n", "max(x)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["For more information, you can follow this [link](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb)"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 1.4. Debugging\n", "\n", "Whenever an exception occurs, the call stack is print out to help you to track down the true source of the problem. It is important to gain familiarity with the call stack, especially when using the PySpark API."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": ["for i in [4,3,2,0]:\n", "    print(5/i)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 1.5. Additional features\n", "\n", "Jupyter also supports viewing the status of the cluster and interact with the real shell environment.\n", "\n", "To do that, you can click on the Logo Jupyter in the up-left coner of each notebook to go to the dashboard:\n", "\n", "<img src=\"https://farm2.staticflickr.com/1488/24681339931_733acb3494_b.jpg\" width=\"600px\" />\n", "\n", "You can easily find out how to use these features, so you're invited to play around!!"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 2. Python + Pandas + Matplotlib: A great environment for Data Science\n", "\n", "This section aims to help the students have a basic understanding of the python programming language and its wonderful libraries. It also helps whose who are not familiar with `Pandas` or `Matplotlib` to have a first glance at basic use of such libraries. \n", "\n", "When working with a small dataset (one that can comfortably fit into a single machine), Pandas and Matplotlib, together with Python are valid alternatives to other popular tools such as R and Matlab. Using such libraries allows to inherit from the simple and clear Python syntax, achieve very good performance, enjoy a better memory management, better error handling, and good package management \\[[1](http://ajminich.com/2013/06/22/9-reasons-to-switch-from-matlab-to-python/)\\].\n", "\n", "\n", "## 2.1. Python syntax\n", "\n", "(This section is for students who did not program in Python before. If you're familiar with Python, please move to the next section: 1.2. Numpy)\n", "\n", "When working with Python, the code seems to be simpler than (many) other languages. In this laboratory, we compare the Python syntax to that of Java - another very common language.\n", "\n", "```java\n", "// java syntax\n", "int i = 10;\n", "string s = \"advanced machine learning\";\n", "System.out.println(i);\n", "System.out.println(s);\n", "// you must not forget the semicolon at the end of each sentence\n", "```"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 10, "cell_type": "code", "source": ["# python syntax\n", "i = 10\n", "s = \"advanced machine learning\"\n", "print(i)\n", "print(s)\n", "# forget about the obligation of commas"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Indentation & If-else syntax\n", "In python, we don't use `{` and `}` to make blocks of codes. Instead, we use indentation to do that. **The codes in the same block must have the same indentation**. For example, in java, we write:\n", "```java\n", "string language = \"Python\";\n", "\n", "// the block is surrounded by { and }\n", "// the condition is in ( and )\n", "if (language == \"Python\") {\n", "    int x = 1;\n", "    x += 10;\n", "       int y = 5; // a wrong indentation isn't problem\n", "    y = x + y;\n", "    System.out.println(x + y);\n", "    \n", "    // a statement is broken into two line\n", "    x = y\n", "        + y;\n", "    \n", "    // do some stuffs\n", "}\n", "else if (language == \"Java\") {\n", "    // another block\n", "}\n", "else {\n", "    // another block\n", "}\n", "```"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 11, "cell_type": "code", "source": ["language = \"Python\"\n", "if language == \"Python\":\n", "    x = 10\n", "    x += 10\n", "    y = 5 # all statements in the same block must has the same indentation\n", "    y = (\n", "        x + y\n", "    ) # a statement can be in multiple line with ( )\n", "    print (x \n", "           + y)\n", "    \n", "    # statement can also be divided by using \\ at the END of each line\n", "    x = y \\\n", "        + y\n", "    \n", "    # do some other stuffs\n", "elif language == \"Java\":\n", "    # another block\n", "    pass\n", "else:\n", "    # another block\n", "    pass"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Ternary conditional operator\n", "In python, we often see ternary conditional operator when reading code of labraries. It is an operator to assign a value for a variable based on some condition. For example, in java, we write:\n", "\n", "```java\n", "int x = 10;\n", "// if x > 10, assign y = 5, otherwise, y = 15\n", "int y = (x > 10) ? 5 : 15;\n", "\n", "int z;\n", "if (x > 10)\n", "    z = 5; // it's not necessary to have { } when the block has only one statement\n", "else\n", "    z = 15;\n", "```\n", "\n", "Of course, although we can easily write these lines of code in an `if else` block to get the same result, people prefer ternary conditioinal operator because of its simplicity.\n", "\n", "In python, we write:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 12, "cell_type": "code", "source": ["x = 10\n", "# a very natural way\n", "y = 5 if x > 10 else 15\n", "print(y)\n", "\n", "# another way\n", "y = x > 10 and 5 or 15\n", "print(y)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### List & For loop\n", "Another syntax that we should revisit is the `for loop`. In java, we can write:\n", "\n", "```java\n", "// init an array with 10 integer numbers\n", "int[] array = new int[]{1, 2, 3, 4, 5, 6, 7, 8, 9, 10};\n", "for (int i = 0; i < array.length; i++){\n", "    // print the i-th element of array\n", "    System.out.println(array[i]);\n", "}\n", "```\n", "\n", "In Python, instead of using an index to help indicating an element, we can access the element directly:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 13, "cell_type": "code", "source": ["array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n", "# Python has no built-in array data structure\n", "# instead, it uses \"list\" which is much more general \n", "# and can be used as a multidimensional array quite easily.\n", "for element in array:\n", "    print(element)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["As we can see, the code is very clean. If you need the index of each element, it's no problem:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 14, "cell_type": "code", "source": ["for (index, element) in enumerate(array):\n", "    print(index, element)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Actually, Python has no built-in array data structure. It uses `list` which is much more general and can be used as a multidimensional array quite easily. Besides, the elements in a list are retrieved in a very concise way. For example, we create a 2d-array with 4 rows. Each row has 3 elements."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 15, "cell_type": "code", "source": ["# 2-dimentions array with 4 rows, 3 columns\n", "twod_array = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n", "for index, row in enumerate(twod_array):\n", "    print(\"row \", index, \":\", row)\n", "\n", "# print row 1 until row 3\n", "print(\"row 1 until row 3: \", twod_array[1:3])\n", "\n", "# all rows from row 2\n", "print(\"all rows from row 2: \", twod_array[2:])\n", "\n", "# all rows until row 2\n", "print(\"all rows until row 2:\", twod_array[:2])\n", "\n", "# all rows from the beginning with step of 2. \n", "print(\"all rows from the beginning with step of 2:\", twod_array[::2])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Dictionary\n", "Another useful data structure in Python is `dictionary`. A dictionary stores (key, value) pairs. You can use it like this:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 16, "cell_type": "code", "source": ["d = {'key1': 'value1', 'key2': 'value2'}  # Create a new dictionary with some data\n", "print(d['key1'])       # Get an entry from a dictionary; prints \"value1\"\n", "print('key1' in d)     # Check if a dictionary has a given key; prints \"True\"\n", "d['key3'] = 'value3'    # Set an entry in a dictionary\n", "print(d['key3'])      # Prints \"wet\"\n", "# print(d['key9'])  # KeyError: 'monkey' not a key of d\n", "print(d.get('key9', 'custom_default_value'))  # Get an element with a default; prints \"custom_default_value\"\n", "print(d.get('key3', 'custom_default_value'))    # Get an element with a default; prints \"value3\"\n", "del d['key3']        # Remove an element from a dictionary\n", "print(d.get('key3', 'custom_default_value')) # \"fish\" is no longer a key; prints \"custom_default_value\"\n"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Functions\n", "In Python, we can define a function by using keyword `def`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 17, "cell_type": "code", "source": ["def square(x):\n", "    return x*x\n", "\n", "print(square(5))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["You can apply a function on each element of a list/array by using `lambda` function. For example, we want to square elements in a list:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 18, "cell_type": "code", "source": ["array = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n", "\n", "# apply function \"square\" on each element of \"array\"\n", "print(list(map(lambda x: square(x), array)))\n", "\n", "# or using a for loop\n", "print([square(x) for x in array])\n", "\n", "print(\"orignal array:\", array)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["These two above syntaxes are used very often. We can also put a function `B` inside a function `A` (that is, we can have nested functions). In that case, function `B` is only accessed inside function `A` (the scope that it's declared). For example:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 19, "cell_type": "code", "source": ["# select only the prime number in array\n", "# and square them\n", "def filterAndSquarePrime(arr):\n", "    \n", "    # a very simple function to check a number is prime or not\n", "    def checkPrime(number):\n", "        for i in range(2, int(number/2)):\n", "            if number % i == 0:\n", "                return False\n", "        return True\n", "    \n", "    primeNumbers = filter(lambda x: checkPrime(x), arr)\n", "    return map(lambda x: square(x), primeNumbers)\n", "\n", "# we can not access checkPrime from here\n", "# checkPrime(5)\n", "\n", "result = filterAndSquarePrime(array)\n", "list(result)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### Importing modules, functions\n", "Modules in Python are packages of code. Putting code into modules helps increasing the reusability and maintainability.\n", "The modules can be nested.\n", "To import a module, we simple use syntax: `import <module_name>`. Once it is imported, we can use any functions, classes inside it."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 20, "cell_type": "code", "source": ["# import module 'math' to uses functions for calculating\n", "import math\n", "\n", "# print the square root of 16\n", "print(math.sqrt(16))\n", "\n", "# we can create alias when import a module\n", "import numpy as np\n", "\n", "print(np.sqrt(16))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Sometimes, you only need to import some functions inside a module to avoid loading the whole module into memory. To do that, we can use syntax: `from <module> import <function>`"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": ["# only import function 'sin' in package 'math'\n", "from math import sin\n", "\n", "# use the function\n", "print(sin(60))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["That's quite enough for Python. Now, let's practice a little bit.\n", "\n", "![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "### Question 1\n", "#### Question 1.1\n", "Write a function `checkSquareNumber` to check if a integer number is a square number or not. For example, 16 and 9 are square numbers. 15 isn't square number.\n", "Requirements:\n", "\n", "- Input: an integer number\n", "\n", "- Output: `True` or `False`\n", "\n", "HINT: If the quare root of a number is an integer number, it is a square number."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "import math\n", "\n", "def checkSquareNumber(x):\n", "    # calculate the square root of x\n", "    # return True if square root is integer, \n", "    # otherwise, return False\n", "    return ...\n", "\n", "print(checkSquareNumber(16))\n", "print(checkSquareNumber(250))\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 1.2\n", "A list `list_numbers` which contains the numbers from 1 to 9999 can be constructed from: \n", "\n", "```python\n", "list_numbers = range(0, 10000)\n", "```\n", "\n", "Extract the square numbers in `list_numbers` using function `checkSquareNumber` from question 1.1. How many elements in the extracted list ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "list_numbers = ...\n", "square_numbers = # try to use the filter method\n", "print(square_numbers)\n", "print(len(square_numbers))\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 1.3\n", "\n", "Using array slicing, select the elements of the list square_numbers, whose index is from 5 to 20 (zero-based index)."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(square_numbers[...])\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["Next, we will take a quick look on Numpy - a powerful module of Python."], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.2. Numpy\n", "Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays.\n", "### 2.2.1. Array\n", "A numpy array is a grid of values, all of **the same type**, and is indexed by a tuple of nonnegative integers. Thanks to the same type property, Numpy has the benefits of [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference). Besides, many other Numpy operations are implemented in C, avoiding the general cost of loops in Python, pointer indirection and per-element dynamic type checking. So, the speed of Numpy is often faster than using built-in datastructure of Python. When working with massive data with computationally expensive tasks, you should consider to use Numpy. \n", "\n", "The number of dimensions is the `rank` of the array; the `shape` of an array is a tuple of integers giving the size of the array along each dimension.\n", "\n", "We can initialize numpy arrays from nested Python lists, and access elements using square brackets:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 25, "cell_type": "code", "source": ["import numpy as np\n", "\n", "# Create a rank 1 array\n", "rank1_array = np.array([1, 2, 3])\n", "print(\"type of rank1_array:\", type(rank1_array))\n", "print(\"shape of rank1_array:\", rank1_array.shape)\n", "print(\"elements in rank1_array:\", rank1_array[0], rank1_array[1], rank1_array[2])\n", "\n", "# Create a rank 2 array\n", "rank2_array = np.array([[1,2,3],[4,5,6]])\n", "print(\"shape of rank2_array:\", rank2_array.shape)\n", "print(rank2_array[0, 0], rank2_array[0, 1], rank2_array[1, 0])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### 2.2.2. Array slicing\n", "Similar to Python lists, numpy arrays can be sliced. The different thing is that you must specify a slice for each dimension of the array because arrays may be multidimensional."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 26, "cell_type": "code", "source": ["m_array = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n", "\n", "# Use slicing to pull out the subarray consisting of the first 2 rows\n", "# and columns 1 and 2\n", "b = m_array[:2, 1:3]\n", "print(b)\n", "\n", "# we can only this syntax with numpy array, not python list\n", "print(\"value at row 0, column 1:\", m_array[0, 1])\n", "\n", "# Rank 1 view of the second row of m_array  \n", "print(\"the second row of m_array:\", m_array[1, :])\n", "\n", "# print element at position (0,2) and (1,3)\n", "print(m_array[[0,1], [2,3]])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### 2.2.3. Boolean array indexing\n", "We can use boolean array indexing to check whether each element in the array satisfies a condition or use it to do filtering."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 27, "cell_type": "code", "source": ["m_array = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n", "\n", "# Find the elements of a that are bigger than 2\n", "# this returns a numpy array of Booleans of the same\n", "# shape as a, where each value of bool_idx tells\n", "# whether that element of a is > 3 or not\n", "bool_idx = (m_array > 3)\n", "print(bool_idx , \"\\n\")\n", "\n", "# We use boolean array indexing to construct a rank 1 array\n", "# consisting of the elements of a corresponding to the True values\n", "# of bool_idx\n", "print(m_array[bool_idx], \"\\n\")\n", "\n", "# We can combine two statements\n", "print(m_array[m_array > 3], \"\\n\")\n", "\n", "# select elements with multiple conditions\n", "print(m_array[(m_array > 3) & (m_array % 2 == 0)])\n"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### 2.2.4. Datatypes\n", "Remember that the elements in a numpy array have the same type. When constructing arrays, Numpy tries to guess a datatype when you create an array However, we can specify the datatype explicitly via an optional argument."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 28, "cell_type": "code", "source": ["# let Numpy guess the datatype\n", "x1 = np.array([1, 2])\n", "print(x1.dtype)\n", "\n", "# force the datatype be float64\n", "x2 = np.array([1, 2], dtype=np.float64)\n", "print(x2.dtype)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["### 2.2.5. Array math\n", "Similar to Matlab or R, in Numpy, basic mathematical functions operate elementwise on arrays, and are available both as operator overloads and as functions in the numpy module."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 29, "cell_type": "code", "source": ["x = np.array([[1,2],[3,4]], dtype=np.float64)\n", "y = np.array([[5,6],[7,8]], dtype=np.float64)\n", "# mathematical function is used as operator\n", "print(\"x + y =\", x + y, \"\\n\")\n", "\n", "# mathematical function is used as function\n", "print(\"np.add(x, y)=\", np.add(x, y), \"\\n\")\n", "\n", "# Unlike MATLAB, * is elementwise multiplication\n", "# not matrix multiplication\n", "print(\"x * y =\", x * y , \"\\n\")\n", "print(\"np.multiply(x, y)=\", np.multiply(x, y), \"\\n\")\n", "print(\"x*2=\", x*2, \"\\n\")\n", "\n", "# to multiply two matrices, we use dot function\n", "print(\"x.dot(y)=\", x.dot(y), \"\\n\")\n", "print(\"np.dot(x, y)=\", np.dot(x, y), \"\\n\")\n", "\n", "# Elementwise square root\n", "print(\"np.sqrt(x)=\", np.sqrt(x), \"\\n\")"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Note that unlike MATLAB, `*` is elementwise multiplication, not matrix multiplication. We instead use the `dot` function to compute inner products of vectors, to multiply a vector by a matrix, and to multiply matrices."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 30, "cell_type": "code", "source": ["# declare two vectors\n", "v = np.array([9,10])\n", "w = np.array([11, 12])\n", "\n", "# Inner product of vectors\n", "print(\"v.dot(w)=\", v.dot(w))\n", "print(\"np.dot(v, w)=\", np.dot(v, w))\n", "\n", "# Matrix / vector product\n", "print(\"x.dot(v)=\", x.dot(v))\n", "print(\"np.dot(x, v)=\", np.dot(x, v))\n", "\n", "# Matrix / matrix product\n", "print(\"x.dot(y)=\", x.dot(y))\n", "print(\"np.dot(x, y)=\", np.dot(x, y))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Besides, we can do other aggregation computations on arrays such as `sum`, `nansum`, or `T`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 31, "cell_type": "code", "source": ["x = np.array([[1,2], [3,4]])\n", "\n", "# Compute sum of all elements\n", "print(np.sum(x))\n", "\n", "# Compute sum of each column\n", "print(np.sum(x, axis=0))\n", "\n", "# Compute sum of each row\n", "print(np.sum(x, axis=1))\n", "\n", "# transpose the matrix\n", "print(x.T)\n", "\n", "# Note that taking the transpose of a rank 1 array does nothing:\n", "v = np.array([1,2,3])\n", "print(v.T)  # Prints \"[1 2 3]\""], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "### Question 2\n", "\n", "Given a 2D array:\n", "\n", "```\n", " 1  2  3  4\n", " 5  6  7  8 \n", " 9 10 11 12\n", "13 14 15 16\n", "```\n", "\n", "#### Question 2.1\n", "\n", "Print the all odd numbers in this array using `Boolean array indexing`."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "array_numbers = np.array([\n", "        [1, 2, 3, 4],\n", "        [5, 6, 7, 8],\n", "        [9, 10, 11, 12],\n", "        [13, 14, 15, 16]\n", "    ])\n", "\n", "print(...)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 2.2\n", "\n", "Extract the second row and the third column in this array using `array slicing`."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(array_numbers[...])\n", "print(array_numbers[...])\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 2.3\n", "Calculate the sum of diagonal elements."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "sum = 0\n", "for i in range(0, ...):\n", "    sum += array_numbers...\n", "    \n", "print(sum)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 2.4\n", "Print elementwise multiplication of the first row and the last row using numpy's functions.\n", "\n", "Print the inner product of these two rows.\n"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(...)\n", "print(...)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.3. Matplotlib\n", "\n", "As its name indicates, Matplotlib is a plotting library. It provides both a very quick way to visualize data from Python and publication-quality figures in many formats. The most important function in matplotlib is `plot`, which allows you to plot 2D data."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 36, "cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "plt.plot([1,2,3,4])\n", "plt.ylabel('custom y label')\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["In this case, we provide a single list or array to the `plot()` command, matplotlib assumes it is a sequence of y values, and automatically generates the x values for us. Since python ranges start with 0, the default x vector has the same length as y but starts with 0. Hence the x data are [0,1,2,3].\n", "\n", "In the next example, we plot figure with both x and y data. Besides, we want to draw dashed lines instead of the solid in default."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 37, "cell_type": "code", "source": ["plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'r--')\n", "plt.show()\n", "\n", "plt.bar([1, 2, 3, 4], [1, 4, 9, 16], align='center')\n", "# labels of each column bar\n", "x_labels = [\"Type 1\", \"Type 2\", \"Type 3\", \"Type 4\"]\n", "# assign labels to the plot\n", "plt.xticks([1, 2, 3, 4], x_labels)\n", "\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["If we want to merge two figures into a single one, subplot is the best way to do that. For example, we want to put two figures in a stack vertically, we should define a grid of plots with 2 rows and 1 column. Then, in each row, a single figure is plotted."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 38, "cell_type": "code", "source": ["# Set up a subplot grid that has height 2 and width 1,\n", "# and set the first such subplot as active.\n", "plt.subplot(2, 1, 1)\n", "plt.plot([1, 2, 3, 4], [1, 4, 9, 16], 'r--')\n", "\n", "# Set the second subplot as active, and make the second plot.\n", "plt.subplot(2, 1, 2)\n", "plt.bar([1, 2, 3, 4], [1, 4, 9, 16])\n", "\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["For more examples, please visit the [homepage](http://matplotlib.org/1.5.1/examples/index.html) of Matplotlib."], "cell_type": "markdown", "metadata": {}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "###  Question 3\n", "Given a list of numbers from 0 to 9999.\n", "\n", "#### Question 3.1\n", "Calculate the histogram of numbers divisible by 3, 7, 11 in the list respectively.\n", "\n", "( Or in other word, how many numbers divisible by 3, 7, 11 in the list respectively ?)"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "arr = np.array(...)\n", "divisors = [3, 7, 11]\n", "histogram = list(...)\n", "print(histogram)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 3.2\n", "Plot the histogram in a line chart."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "# simple line chart\n", "plt.plot(histogram)\n", "x_indexes = ...\n", "x_names = list(...)\n", "plt.xticks(x_indexes, x_names)\n", "plt.show()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 3.3\n", "Plot the histogram in a bar chart."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "# char chart with x-lables\n", "x_indexes = range(...)\n", "x_names = list(...)\n", "plt.bar( x_indexes, histogram, align='center')\n", "plt.xticks(x_indexes, x_names)\n", "plt.show()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["## 2.4. Pandas\n", "\n", "Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Indeeed, it is great for data manipulation, data analysis, and data visualization.\n", "\n", "### 2.4.1. Data structures\n", "Pandas introduces has two useful (and powerful) structures: `Series` and `DataFrame`, both of which are built on top of NumPy.\n", "\n", "#### Series\n", "A `Series` is a one-dimensional object similar to an array, list, or even column in a table. It assigns a *labeled index* to each item in the Series. By default, each item will receive an index label from `0` to `N-1`, where `N` is the number items of `Series`.\n", "\n", "We can create a Series by passing a list of values, and let pandas create a default integer index.\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 42, "cell_type": "code", "source": ["import pandas as pd\n", "import numpy as np\n", "\n", "# create a Series with an arbitrary list\n", "s = pd.Series([3, 'Machine learning', 1.414259, -65545, 'Happy coding!'])\n", "print(s)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Or, an index can be used explixitly when creating the `Series`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 43, "cell_type": "code", "source": ["s = pd.Series([3, 'Machine learning', 1.414259, -65545, 'Happy coding!'],\n", "             index=['Col1', 'Col2', 'Col3', 4.1, 5])\n", "print(s)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["A `Series` can be constructed from a dictionary too."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 44, "cell_type": "code", "source": ["s = pd.Series({\n", "        'Col1': 3, 'Col2': 'Machine learning', \n", "        'Col3': 1.414259, 4.1: -65545, \n", "        5: 'Happy coding!'\n", "    })\n", "print(s)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can access items in a `Series` in a same way as `Numpy`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 45, "cell_type": "code", "source": ["s = pd.Series({\n", "        'Col1': 3, 'Col2': -10, \n", "        'Col3': 1.414259, \n", "        4.1: -65545, \n", "        5: 8\n", "    })\n", "\n", "# get element which has index='Col1'\n", "print(\"s['Col1']=\", s['Col1'], \"\\n\")\n", "\n", "# get elements whose index is in a given list\n", "print(\"s[['Col1', 'Col3', 4.5]]=\", s[['Col1', 'Col3', 4.5]], \"\\n\")\n", "\n", "# use boolean indexing for selection\n", "print(s[s > 0], \"\\n\")\n", "\n", "# modify elements on the fly using boolean indexing\n", "s[s > 0] = 15\n", "\n", "print(s, \"\\n\")\n", "\n", "# mathematical operations can be done using operators and functions.\n", "print(s*10,  \"\\n\")\n", "print(np.square(s), \"\\n\")"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["#### DataFrame\n", "A DataFrame is a tablular data structure comprised of rows and columns, akin to database table, or R's data.frame object. In a loose way, we can also think of a DataFrame as a group of Series objects that share an index (the column names).\n", "\n", "We can create a DataFrame by passing a dict of objects that can be converted to series-like."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 46, "cell_type": "code", "source": ["data = {'year': [2013, 2014, 2015, 2013, 2014, 2015, 2013, 2014],\n", "        'team': ['Manchester United', 'Chelsea', 'Asernal', 'Liverpool', 'West Ham', 'Newcastle', 'Machester City', 'Tottenham'],\n", "        'wins': [11, 8, 10, 15, 11, 6, 10, 4],\n", "        'losses': [5, 8, 6, 1, 5, 10, 6, 12]}\n", "football = pd.DataFrame(data, columns=['year', 'team', 'wins', 'losses'])\n", "football"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can store data as a CSV file, or read data from a CSV file."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 47, "cell_type": "code", "source": ["# save data to a csv file without the index\n", "football.to_csv('football.csv', index=False)\n", "\n", "from_csv = pd.read_csv('football.csv')\n", "from_csv.head()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["To read a CSV file with a custom delimiter between values and custom columns' names, we can use parameters `sep` and `names` relatively.\n", "Moreover, Pandas also supports to read and write to [Excel file](http://pandas.pydata.org/pandas-docs/stable/io.html#io-excel) , sqlite database file, URL,  or even clipboard.\n", "\n", "We can have an overview on the data by using functions `info` and `describe`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 48, "cell_type": "code", "source": ["print(football.info(), \"\\n\")\n", "football.describe()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Numpy's regular slicing syntax works as well."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 49, "cell_type": "code", "source": ["print(football[0:2], \"\\n\")\n", "\n", "# query only the teams that win more than 10 matches from 2014\n", "print(football[(football.year >= 2014) & (football.wins >= 10)])"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["An important feature that Pandas supports is `JOIN`. Very often, the data comes from multiple sources, in multiple files. For example, we have 2 CSV files, one contains the information of Artists, the other contains information of Songs. If we want to query the artist name and his/her corresponding songs, we have to do joining two dataframe.\n", "\n", "Similar to SQL, in Pandas, you can do inner join, left outer join, right outer join and full outer join. Let's see a small example. Assume that we have two dataset of singers and songs. The relationship between two datasets is maintained by a constrain on `singer_code`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 50, "cell_type": "code", "source": ["singers = pd.DataFrame({'singer_code': range(5), \n", "                           'singer_name': ['singer_a', 'singer_b', 'singer_c', 'singer_d', 'singer_e']})\n", "songs = pd.DataFrame({'singer_code': [2, 2, 3, 4, 5], \n", "                           'song_name': ['song_f', 'song_g', 'song_h', 'song_i', 'song_j']})\n", "print(singers)\n", "print('\\n')\n", "print(songs)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 51, "cell_type": "code", "source": ["# inner join\n", "pd.merge(singers, songs, on='singer_code', how='inner')"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 52, "cell_type": "code", "source": ["# left join\n", "pd.merge(singers, songs, on='singer_code', how='left')"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 53, "cell_type": "code", "source": ["# right join\n", "pd.merge(singers, songs, on='singer_code', how='right')"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 54, "cell_type": "code", "source": ["# outer join (full join)\n", "pd.merge(singers, songs, on='singer_code', how='outer')"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We can also concat two dataframes vertically or horizontally via function `concat` and parameter `axis`. This function is useful when we need to append two similar datasets or to put them side by site"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 55, "cell_type": "code", "source": ["# concat vertically\n", "pd.concat([singers, songs])"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 56, "cell_type": "code", "source": ["# concat horizontally\n", "pd.concat([singers, songs], axis=1)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["When doing statistic, we usually need to aggregate data by each group. For example, to anwser the question \"how many songs each singer has?\", we have to group data by each singer, and then calculate the number of songs in each group. Not that the result must contain the statistic of all singers in database (even if some of them have no song)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 57, "cell_type": "code", "source": ["data = pd.merge(singers, songs, on='singer_code', how='left')\n", "\n", "# count the values of each column in group\n", "print(data.groupby('singer_code').count())\n", "\n", "print(\"\\n\")\n", "\n", "# count only song_name\n", "print(data.groupby('singer_code').song_name.count())\n", "\n", "print(\"\\n\")\n", "\n", "# count song name but ignore duplication, and order the result\n", "print(data.groupby('singer_code').song_name.nunique().sort_values(ascending=True))"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "\n", "### Question 4\n", "\n", "We have two datasets about music: [song](https://github.com/michiard/AML-COURSE/blob/master/data/song.tsv) and [album](https://github.com/michiard/AML-COURSE/blob/master/data/album.tsv).\n", "\n", "In the following questions, you **have to** use Pandas to load data and write code to answer these questions.\n", "\n", "#### Question 4.1\n", "Load both dataset into two dataframes and print the information of each dataframe\n", "\n", "**HINT**: \n", "\n", "- You can click button `Raw` on the github page of each dataset and copy the URL of the raw file.\n", "- The dataset can be load by using function `read_table`. For example: `df = pd.read_table(raw_url, sep='\\t')`"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "import pandas as pd\n", "\n", "songdb_url = 'https://raw.githubusercontent.com/DistributedSystemsGroup/Algorithmic-Machine-Learning/master/data/song.tsv'\n", "albumdb_url = 'https://raw.githubusercontent.com/DistributedSystemsGroup/Algorithmic-Machine-Learning/master/data/album.tsv'\n", "song_df = pd...\n", "album_df = pd...\n", "\n", "print(song_df...)\n", "print(album_df...)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 4.2\n", "How many albums in this datasets ?\n", "\n", "How many songs in this datasets ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(\"number of albums:\", album_df....count())\n", "print(\"number of songs:\", song_df.Song...)\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 4.3\n", "How many distinct singers in this dataset ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "print(\"number distinct singers:\", len(...))\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "#### Question 4.4\n", "Is there any song that doesn't belong to any album ?\n", "\n", "Is there any album that has no song ?\n", "\n", "**HINT**: \n", "\n", "- To join two datasets on different key names, we use `left_on=` and `right_on=` instead of `on=`.\n", "- Funtion `notnull` and `isnull` help determining the value of a column is missing or not. For example:\n", "`df['song'].isnull()`."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "fulldf = pd.merge(song_df, album_df, how='outer', left_on='Album', right_on='Album code')\n", "fulldf[fulldf['Song'].... & fulldf['Album']....]\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "fulldf[fulldf['Song'].... & fulldf['Album code']....]\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "#### Question 4.5\n", "How many songs in each albums of Michael Jackson ?"], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "\n", "# Try thinking like as for map reduce word count!!\n", "\n", "fulldf[fulldf['Singer']=='Michael Jackson']....\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 3. PySpark\n", "\n", "Spark is an open source alternative to MapReduce designed to make it easier to build and run fast data manipulation on Hadoop. Spark comes with a library of machine learning (ML) and graph algorithms, and also supports real-time streaming and SQL apps, via Spark Streaming and Shark, respectively. Spark exposes the Spark programming model to Java, Scala, or Python. In Python, we use PySpark API to interact with Spark.\n", "\n", "As discussed in the introductory lecture, every Spark application has a Spark driver. It is the program that declares the transformations and actions on RDDs of data and submits such requests to the cluster manager. Actually, the driver is the program that creates the `SparkContext`, connecting to a given cluster manager such as  Spark Master, YARN cluster manager\\[[2](http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/)\\]... The executors run user code, run computations and can cache data for your application. The `SparkContext` will create a job that is broken into stages. The stages are broken into tasks which are scheduled by the SparkContext on an executor.\n", "\n", "![](http://blogs.msdn.com/cfs-file.ashx/__key/communityserver-blogs-components-weblogfiles/00-00-01-61-78-metablogapi/3566.091415_5F00_1429_5F00_Understandi1.png)\n", "\n", "When starting PySpark with command `pyspark` or using a well configurated notebook (such as this one), `SparkContext` is created automatically in variable `sc`. \n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 64, "cell_type": "code", "source": ["sc"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["PySpark uses PySpark RDDs which  are just RDDs of Python objects: like Python lists, they can store objects with mixed types (actually all the objects are instances of `PyObject`).\n", "\n", "When PySpark is started, it also starts a JVM, which is accessible through a socket. PySpark uses the `Py4J` project to handle this communication. The JVM works as the actual Spark driver, and loads a `JavaSparkContext` that communicates with the Spark executors across the cluster. Python API calls to the Spark Context object are then **translated into Java API calls** to the JavaSparkContext. For example, the implementation of PySpark's `sc.textFile()` dispatches a call to the `.textFile` method of the `JavaSparkContext`, which ultimately communicates with the Spark executor JVMs to load the text data from HDFS. \n", "\n", "![](http://i.imgur.com/YlI8AqEl.png)\n", "\n", "The Spark executors on the cluster start a Python interpreter for each core, with which they communicate data through a pipe when they need to execute user-code. A Python RDD in the local PySpark client corresponds to a `PythonRDD` object in the local JVM. The data associated with the RDD actually lives in the Spark JVMs as Java objects. For example, running `sc.textFile()` in the Python interpreter will call the `JavaSparkContexts` `textFile` method, which loads the data as Java String objects in the cluster.\n", "\n", "\n", "When an API call is made on the `PythonRDD`, any associated code (e.g., Python lambda function) **is serialized and distributed to the executors**. The data is then converted from Java objects to a Python-compatible representation (e.g., pickle objects) and streamed to executor-associated Python interpreters through a pipe. Any necessary Python processing is executed in the interpreter, and the resulting data is stored back as an RDD (as pickle objects by default) in the JVMs. \n"], "cell_type": "markdown", "metadata": {}}, {"source": ["The data is read easily by using functions of Spark Context. For example, to read a text file and count the number of lines, we can write:\n"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 65, "cell_type": "code", "source": ["# each line is stored as an element in 'words' - a PythonRDD.\n", "words = sc.textFile(\"/datasets/textfile\")\n", "num_lines = words.count()\n", "print(\"the number of lines in file\", num_lines)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 3.1. Wordcount example\n", "In the below example, we try to select top 10 words which has occurred the most in a text file and plot them using Matplotlib.\n", "\n", "To do this exercise, we go through the workflow to understand what we should do. First, using method `textFile` of SparkContext `sc`, we create a RDD of string. Each string in RDD is representative for a line in the text file. In a loose way, we can think the first RDD is a RDD of lines. \n", "\n", "Because we work on the scope of words, we have to transform **a line** in the current RDD into **multiple words**, each word is an object of the new RDD. This is done by using `flatMap` function. \n", "\n", "Then, a `map` function will transform **each word** in RDD into **a** tuple with 2 components: the word itselft and 1. At this time, each object of the RDD is actually a key-value pair. Number 1 here mean that we have encountered one time. \n", "\n", "We can take advantage of function `reduceByKey` to sum all frequencies of the same word. Now, each element in the RDD is in form of: (word, total_frequency). To sort the words by its frequency, we can have many ways. One of the simplest approach is swap each tuple such that the frequency will become the key and then use `sortByKey` function."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 66, "cell_type": "code", "source": ["words = (\n", "            # read the text file\n", "            sc.textFile(\"/datasets/textfile\")\n", "            \n", "            # construct words from lines\n", "            .flatMap(lambda line: line.split())\n", "            \n", "            # map each word to (word, 1)\n", "            .map(lambda x: (x, 1))\n", "    \n", "            # reduce by key: accumulate sum the freq of the same word\n", "            .reduceByKey(lambda freq1, freq2: freq1 + freq2)\n", "            \n", "            # swap (word, freq) to (freq, word)\n", "            .map(lambda x: (x[1], x[0]))\n", "    \n", "            # sort result by key DESC\n", "            .sortByKey(False)\n", "         )"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Now the top-10 words are collected and sent back to the driver by using function `take`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 67, "cell_type": "code", "source": ["# top 10 words:\n", "top10 = words.take(10)\n", "print(top10)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["The function `collect` will sent all elements in the RDD to the driver as an list."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 68, "cell_type": "code", "source": ["# collect results from executors to the driver\n", "results = words.collect()\n", "print(results)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["There are two type of functions in Spark: **transformation** and **action**. All functions `map`, `flatMap`, `reduceByKey`, `sortByKey` are transformation functions. They are not executed right away when called. Indeed, Spark is lazy, so nothing will get executed unless you call some actions such as `count`, `take`, `collect`...\n", "\n", "RDD transformations allow us to create dependencies between RDDs. Dependencies are only steps for producing results. Each RDD in lineage chain (string of dependencies) has a function for calculating its data and has a pointer (dependency) to its parent RDD. Everytime we use an RDD, its dependencies is calculated again from beginning. In many cases, that does not take advantage of the pre-computed results. Fortunatly, we can use function `cache` to make a checkpoint for a RDD. Actually, the data of cached RDD can be stored in memory, or disk."], "cell_type": "markdown", "metadata": {}}, {"source": ["We have a result for our Word Count example. Now, it's time for plotting!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 69, "cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "# extract the frequencies from the result\n", "frequencies = [x[0] for x in top10]\n", "\n", "# plot the frequencies\n", "plt.plot(frequencies)\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 3.2. Night flights example\n", "We have a CSV file which contains the information about flights that took place in the US in 1994.\n", "The data in this file has 29 columns such as `year`, `month`, `day_of_month`, `scheduled_departure_time`,...\n", "We can have a quick look on the data:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 70, "cell_type": "code", "source": ["! hdfs dfs -cat /datasets/airline/1994.csv | head -n 10"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["In this example, we only take care about columns `CRSDepTime` (scheduled departure time) and `UniqueCarrier` (carrier of flight). The values of `CRSDepTime` is in format of: hhmm (hour-minute).\n", "Assume that a flight is considered as 'night flight' if its scheduled departured time is late than 18:00.\n", "\n", "Questions:\n", "\n", "- How many night flights in the data ?\n", "- How many night flights of each unique carrier ? Plot top 5 of them.\n", "\n", "First, we read the data and remove the header. Then, from the lines, we extract the information of scheduled departure time and carrier."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 71, "cell_type": "code", "source": ["# read the data\n", "data = sc.textFile('/datasets/airline/1994.csv')\n", "\n", "# extract information about scheduled departure time and carrier\n", "# note that the scheduled time must be convert from string to interger number\n", "def extract_CRSDepTime_Carier(line):\n", "    cols = line.split(\",\")\n", "    return (int(cols[5]), cols[8])\n", "\n", "header = data.first()\n", "\n", "# remove header\n", "data_without_header = data.filter(lambda line: line != header)\n", "\n", "# screate RDD with only scheduled departure time and carrier information\n", "# cache it for later usages\n", "newdata = (\n", "            data_without_header\n", "               .map(extract_CRSDepTime_Carier)\n", "               .cache()\n", "          )\n"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Function `filter` helps us select only the objects that satisfy a condition. In this case, it creates a new RDD by filtering out the header. We can also use it to select the night flights."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 72, "cell_type": "code", "source": ["night_flights = newdata.filter(lambda f: f[0] > 1800).cache()\n", "night_flights.take(3)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We use `cache` because we dont want to recalculate `night_flights` from the beginning everytime of using it."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 73, "cell_type": "code", "source": ["# filter and count the night flights\n", "num_night_flights = night_flights.count()\n", "print(num_night_flights)"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 74, "cell_type": "code", "source": ["# group by carrier\n", "night_flights_by_carrier = night_flights.groupBy(lambda x: x[1]).mapValues(lambda flights: len(flights))\n", "\n", "# take top 5 carriers\n", "top5_carriers = night_flights_by_carrier.takeOrdered(5, key=lambda x: -x[1])\n", "\n", "print(top5_carriers)"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["We use `groupBy` to put all flights which belong to the same carrier into a group. In this example, to select top 5 carriers, we don't swap key-value pairs anymore. Alternatively, `takeOrder` can handle that. This function will take top `k` objects ordered by the index. The trick is that we ask it to use the new key, instead of the current one (the carrier).\n", "\n", "Let's plot a bar char from the result by Matplotlib. To draw a bar char, we use function `bar` which requires two parameters. Each parameter is a list of float values in each dimension."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 75, "cell_type": "code", "source": ["%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "# extract the number of flights which will be used as y-values\n", "num_flights = [ x[1] for x in top5_carriers]\n", "\n", "# extract the carriers' names\n", "carrier_names = [x[0] for x in top5_carriers]\n", "\n", "# create `virtual indexes for carriers which will be used as x-values`\n", "carrier_indexes = range(0, len(carrier_names))\n", "\n", "# plot\n", "plt.bar(carrier_indexes, num_flights, align=\"center\")\n", "\n", "# put x-labels for the plot\n", "plt.xticks(carrier_indexes, carrier_names)\n", "plt.show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "\n", "\n", "## Question 5\n", "\n", "\n", "### Question 5.1\n", "Calculate how many flights have the scheduled departure time after 09:00 and before 14:00."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "# read the data\n", "data = sc.textFile('/datasets/airline/1994.csv')\n", "\n", "# extract information about scheduled departure time and carrier\n", "# note that the scheduled time must be convert from string to interger number\n", "def extract_CRSDepTime_Carier(line):\n", "    ...\n", "    ...\n", "    return (int(cols[5]), cols[16])\n", "\n", "header = data.first()\n", "\n", "# remove header\n", "data_without_header = data.filter(...)\n", "\n", "# create RDD with only scheduled departure time and carrier information\n", "# cache it for later usages\n", "newdata = (\n", "            data_without_header\n", "               .map(extract_CRSDepTime_Carier)\n", "               ...\n", "          )\n", "\n", "flights = newdata.filter(...).cache()\n", "\n", "print(flights.count())\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "### Question 5.2\n", "Calculate the number flights that have a scheduled departure time after 09:00 and before 14:00, for each source airport (origin). Plot top 5 of them."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "flights_per_carrier = flights.groupBy(...).mapValues(...)\n", "\n", "# take top 5 source airports\n", "top5_source_airport = flights_per_carrier.takeOrdered(...)\n", "\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "# extract the number of flights which will be used as y-values\n", "# This is called list comprehension\n", "num_flights = [ x[1] for x in top5_source_airport]\n", "\n", "# create `virtual indexes for carriers which will be used as x-values`\n", "airport_indexes = range(0, len(top5_source_airport))\n", "\n", "# plot\n", "plt.bar(airport_indexes, num_flights, align=\"center\")\n", "\n", "# extract the carriers' names\n", "airport_names = [ x[0] for x in top5_source_airport]\n", "\n", "# put x-labels for the plot\n", "plt.xticks(airport_indexes, airport_names)\n", "plt.show()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["# 4. Spark SQL and DataFrames\n", "\n", "One of the main modules that we suggest to use when analyzing data with Spark is `Spark SQL` - a module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Internally, this extra information is used to perform extra optimizations. There are several ways to interact with Spark SQL including SQL, the DataFrames API and the Datasets API. In this course, we mainly focus on `DataFrame API`. \n", "\n", "A `DataFrame` is a distributed collection of data organized into named columns. It is based on the data frame concept in R language or in Pandas. So, it is similar to a database table in a relational database.\n", "\n", "`DataFrames` can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs.\n", "\n", "During the lectures in this course, we will mainly work with CSV data file. So, in the next sections, we only focus on constructing dataframes from structured data file directly and from existing RDD.\n", "\n", "## 4.1. Constructing directly from structured data file\n", "\n", "To construct DataFrame from a structured file directly, the file type must be supported such as csv, json, avro...\n", "Among these types, csv type is one of the most popular in data analytic. A DataFrame is constructed from csv files by using package `spark-csv` from Databrick."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 78, "cell_type": "code", "source": ["from pyspark.sql import SQLContext\n", "from pyspark.sql.types import *\n", "\n", "sqlContext = SQLContext(sc)\n", "\n", "df = sqlContext.read.load('/datasets/airline/1994.csv', \n", "                          format='com.databricks.spark.csv', \n", "                          header='true', \n", "                          inferSchema='true',\n", "                          nullValue='NA'\n", "                        )"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["With function `load` and assigning value `com.databricks.spark.csv` for parameter `format`, we ask SqlContext to use the parser from DataBrick's package. Besides, we can specify whether the file has header, or ask the parser to guess the data type of columns automatically. The parsed data types is viewed by function `printSchema`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 79, "cell_type": "code", "source": ["# print(df.dtypes)\n", "df.printSchema()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["Actually, in this case, the inferred data types are not as expected. For example, we expect that `CRSDepTime` has interger type. Fortunately, the type and the name of each column can be changed by using function `withColumn` and `withColumnRename` respectively. Besides, we can also view the basic statistic of numerical columns via function `describe` (similar to Pandas)."], "cell_type": "markdown", "metadata": {}}, {"execution_count": 80, "cell_type": "code", "source": ["\n", "df = (df\n", "          # change type of column CRSDepTime by casting its values to interger type\n", "          .withColumn('CRSDepTime', df.CRSDepTime.cast('int'))\n", "      \n", "          # rename the column\n", "          .withColumnRenamed('CRSDepTime', 'scheduled_departure_time')\n", "    )\n", "\n", "# print schema of the current data\n", "df.printSchema()\n", "\n", "# run jobs to calculate basic statistic information and show it\n", "df.describe().show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 4.2. Constructing from an existing RDD\n", "Another way to construct DataFrame is using data from an existing RDD. The main advantage of this approach is that it does not need a third party library. However, with this method, we have to remove the header ourself and provide a clear schema. "], "cell_type": "markdown", "metadata": {}}, {"execution_count": 81, "cell_type": "code", "source": ["from pyspark.sql import SQLContext\n", "from pyspark.sql.types import *\n", "\n", "sqlContext = SQLContext(sc)\n", "\n", "data = sc.textFile('/datasets/airline/1994.csv')\n", "\n", "# extract the header\n", "header = data.first()\n", "\n", "# replace invalid data with NULL and remove header\n", "cleaned_data = (\n", "        data\n", "    \n", "        # filter out the header\n", "        .filter(lambda line: line != header)\n", "    \n", "         # remove the 'missing data' by empty value\n", "        .map(lambda l: l.replace(',NA', ','))\n", "    )\n", "\n", "airline_data_schema = StructType([ \\\n", "    #StructField( name, dataType, nullable)\n", "    StructField(\"year\",                     IntegerType(), True), \\\n", "    StructField(\"month\",                    IntegerType(), True), \\\n", "    StructField(\"day_of_month\",             IntegerType(), True), \\\n", "    StructField(\"day_of_week\",              IntegerType(), True), \\\n", "    StructField(\"departure_time\",           IntegerType(), True), \\\n", "    StructField(\"scheduled_departure_time\", IntegerType(), True), \\\n", "    StructField(\"arrival_time\",             IntegerType(), True), \\\n", "    StructField(\"scheduled_arrival_time\",   IntegerType(), True), \\\n", "    StructField(\"carrier\",                  StringType(),  True), \\\n", "    StructField(\"flight_number\",            StringType(),  True), \\\n", "    StructField(\"tail_number\",              StringType(), True), \\\n", "    StructField(\"actual_elapsed_time\",      IntegerType(), True), \\\n", "    StructField(\"scheduled_elapsed_time\",   IntegerType(), True), \\\n", "    StructField(\"air_time\",                 IntegerType(), True), \\\n", "    StructField(\"arrival_delay\",            IntegerType(), True), \\\n", "    StructField(\"departure_delay\",          IntegerType(), True), \\\n", "    StructField(\"src_airport\",              StringType(),  True), \\\n", "    StructField(\"dest_airport\",             StringType(),  True), \\\n", "    StructField(\"distance\",                 IntegerType(), True), \\\n", "    StructField(\"taxi_in_time\",             IntegerType(), True), \\\n", "    StructField(\"taxi_out_time\",            IntegerType(), True), \\\n", "    StructField(\"cancelled\",                StringType(),  True), \\\n", "    StructField(\"cancellation_code\",        StringType(),  True), \\\n", "    StructField(\"diverted\",                 StringType(),  True), \\\n", "    StructField(\"carrier_delay\",            IntegerType(), True), \\\n", "    StructField(\"weather_delay\",            IntegerType(), True), \\\n", "    StructField(\"nas_delay\",                IntegerType(), True), \\\n", "    StructField(\"security_delay\",           IntegerType(), True), \\\n", "    StructField(\"late_aircraft_delay\",      IntegerType(), True)\\\n", "])"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 82, "cell_type": "code", "source": ["# convert each line into a tuple of features (columns) with the corresponding data type\n", "cleaned_data_to_columns = (\n", "    cleaned_data.map(lambda l: l.split(\",\"))\n", "    .map(lambda cols: \n", "         (\n", "            int(cols[0])  if cols[0] else None,\n", "            int(cols[1])  if cols[1] else None,\n", "            int(cols[2])  if cols[2] else None,\n", "            int(cols[3])  if cols[3] else None,\n", "            int(cols[4])  if cols[4] else None,\n", "            int(cols[5])  if cols[5] else None,\n", "            int(cols[6])  if cols[6] else None,\n", "            int(cols[7])  if cols[7] else None,\n", "            cols[8]       if cols[8] else None,\n", "            cols[9]       if cols[9] else None,\n", "            cols[10]      if cols[10] else None,\n", "            int(cols[11]) if cols[11] else None,\n", "            int(cols[12]) if cols[12] else None,\n", "            int(cols[13]) if cols[13] else None,\n", "            int(cols[14]) if cols[14] else None,\n", "            int(cols[15]) if cols[15] else None,\n", "            cols[16]      if cols[16] else None,\n", "            cols[17]      if cols[17] else None,\n", "            int(cols[18]) if cols[18] else None,\n", "            int(cols[19]) if cols[19] else None,\n", "            int(cols[20]) if cols[20] else None,\n", "            cols[21]      if cols[21] else None,\n", "            cols[22]      if cols[22] else None,\n", "            cols[23]      if cols[23] else None,\n", "            int(cols[24]) if cols[24] else None,\n", "            int(cols[25]) if cols[25] else None,\n", "            int(cols[26]) if cols[26] else None,\n", "            int(cols[27]) if cols[27] else None,\n", "            int(cols[28]) if cols[28] else None\n", "         ))             \n", ")\n", "    \n", "# create dataframe\n", "df = sqlContext.createDataFrame(cleaned_data_to_columns, airline_data_schema)\\\n", "    .select(['year', 'month', 'day_of_month', 'day_of_week',\n", "            'scheduled_departure_time','scheduled_arrival_time',\n", "            'arrival_delay', 'distance', \n", "            'src_airport', 'dest_airport', 'carrier'])\\\n", "    .cache()"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 83, "cell_type": "code", "source": ["print(df.dtypes)\n", "df.describe().show()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["## 4.3. Night flight example\n", "Using the contructed DataFrame, we can answer the questions about night flights in the previous section:\n", "\n", "- How many night flights in the data ?\n", "- How many night flights of each unique carrier ?"], "cell_type": "markdown", "metadata": {}}, {"execution_count": 84, "cell_type": "code", "source": ["df[df.scheduled_departure_time > 1800].count()"], "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": 85, "cell_type": "code", "source": ["df[df.scheduled_departure_time > 1800].groupBy(df.carrier).count().orderBy('count', ascending=0).collect()"], "outputs": [], "metadata": {"collapsed": false}}, {"source": ["![](https://farm2.staticflickr.com/1604/24934700445_833f0a5649_t.jpg)"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "## Question 6\n", "\n", "\n", "### Question 6.1\n", "Using Spark SQL, calculate how many flights have the scheduled departure time after 09:00 and before 14:00."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "flights = df[(df.scheduled_departure_time > ...) & (df.scheduled_departure_time < ...)]\n", "flights....\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["<div class='anchor' ></div>\n", "### Question 6.2\n", "Calculate the number flights that have the scheduled departure time after 09:00 and before 14:00, for each source airport (origin). Plot top 5 of them."], "cell_type": "markdown", "metadata": {}}, {"source": ["```python\n", "top5_source_airport = flights.groupBy(...).count().orderBy('count', ascending=0).take(5)\n", "\n", "pdf = pd.DataFrame(data=top5_source_airport)\n", "\n", "print(pdf)\n", "\n", "%matplotlib inline\n", "import matplotlib.pyplot as plt\n", "\n", "\n", "# create `virtual indexes for carriers which will be used as x-values`\n", "airport_indexes = range(0, len(top5_source_airport))\n", "\n", "# plot\n", "plt.bar(airport_indexes, pdf[1], align=\"center\")\n", "\n", "# put x-labels for the plot\n", "plt.xticks(airport_indexes, pdf[0])\n", "plt.show()\n", "```"], "cell_type": "markdown", "metadata": {}}, {"source": ["# Summary\n", "\n", "In this lecture, we gained familiarity with the Jupyter Notebook environment, the Python programming language and its modules. In particular, we covered the Python syntax, Numpy - the core library for scientific computing, Matplotlib - a module to plot graphs, Pandas - a data analysis module. Besides, we started to gain practical experience with PySpark and SparkSQL, using as an example a dataset concerning US flights."], "cell_type": "markdown", "metadata": {}}, {"source": ["# References\n", "This notebook is inspired from:\n", "\n", "- [Python Numpy tutorial](http://cs231n.github.io/python-numpy-tutorial/)"], "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3", "name": "python3", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.4.4", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}